model_name_or_path: /data/sjtu/wrx/llamafactory/psychgpt-r1-0313-refine-sft-dpo-task2-knowledge-merge/
adapter_name_or_path: /data/sjtu/wrx/llamafactory/psychgpt-r1-0313-refine-sft-dpo-task2-knowledge-sft-CMBCME/
template: qwen
finetuning_type: lora
# infer_backend: vllm 
temperature: 0.1
max_new_tokens: 4096
# repetition_penalty: 1.2